{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host_Since\n",
      "Calendar_last_Scraped\n",
      "Host_Since\n",
      "Calendar_last_Scraped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 81)\n",
      "(100000, 2)\n",
      "(100000, 81)\n",
      "(100000, 2)\n",
      "(100000, 81)\n",
      "(100000, 2)\n",
      "(100000, 131)\n",
      "(100000, 81)\n",
      "(100000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99992, 223)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = None     ## Columns\n",
    "pd.options.display.max_rows = None        ## Rows\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection # for splitting into train and test\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "from sklearn import linear_model # for logistic model\n",
    "\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_csv('listings.csv', sep = ';')\n",
    "df.loc[df.Listing_Type=='Good','Listing_Type']=1\n",
    "df.loc[df.Listing_Type=='Bad','Listing_Type']=0\n",
    "df= df[0:100000]\n",
    "df.shape\n",
    "\n",
    "\n",
    "text_col=['Name','Summary','Space','Description','Neighborhood_Overview','Notes','Transit','Interaction','House_Rules','Host_About','Amenities']\n",
    "df= df.drop(columns=text_col)\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "## DROP Host_Name , Experiences_Offered , Unnamed: 0 , ID ,\n",
    "# drop due to missing value - Access , Host_Response_Time , Host_Response_Rate ,Host_Acceptance_Rate, Host_Neighbourhood , Neighbourhood ,Neighbourhood_Group_Cleansed,Square_Feet,\n",
    "# Weekly_Price , Monthly_Price , Security_Deposit , Cleaning_Fee , Has_Availability, First_Review , Last_Review , Reviews_per_Month , \n",
    "# Rating \n",
    "\n",
    "miss_val = ['Experiences_Offered' , 'Access' , 'Host_Response_Time' , 'Host_Response_Rate' ,'Host_Acceptance_Rate', 'Host_Neighbourhood' , 'Neighbourhood' ,'Neighbourhood_Group_Cleansed','Square_Feet','Weekly_Price' , 'Monthly_Price' , 'Security_Deposit' , 'Cleaning_Fee' , 'Has_Availability', 'First_Review' , 'Last_Review' , 'Reviews_per_Month' , 'Rating' ]\n",
    "df= df.drop(columns=miss_val)\n",
    "\n",
    "id_col = ['Host_Name'  , 'Unnamed: 0' , 'ID']\n",
    "df= df.drop(columns=id_col)\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "dateList=['Host_Since','Calendar_last_Scraped']\n",
    "df[dateList].head()\n",
    "\n",
    "# converting above 3 columns to datetime (from object type)\n",
    "\n",
    "for dateColumn in dateList:\n",
    "    print(dateColumn)\n",
    "    df[dateColumn] = pd.to_datetime(df[dateColumn],format='%Y-%m-%d',errors='coerce')\n",
    "    \n",
    "\n",
    "for dateColumn in dateList:\n",
    "    print(dateColumn)\n",
    "#     df[dateColumn+'_YEAR']=df[dateColumn].dt.year\n",
    "#     df[dateColumn+'_MONTH']=df[dateColumn].dt.month\n",
    "#     df[dateColumn+'_WEEK']=df[dateColumn].dt.week\n",
    "#     df[dateColumn+'_DAY']=df[dateColumn].dt.day\n",
    "#     df[dateColumn+'_DAYOFWEEK']=df[dateColumn].dt.dayofweek\n",
    "    df[dateColumn+'_DAYSSINCETODAY']=(pd.datetime.now()-df['Host_Since']).dt.days\n",
    "\n",
    "\n",
    "\n",
    "df= df.drop(columns=['Host_Since','Calendar_last_Scraped'])\n",
    "# unwanted_dates = ['Host_Since','Host_Since_YEAR' , 'Host_Since_MONTH', 'Host_Since_WEEK' , 'Host_Since_DAY' , 'Host_Since_DAYOFWEEK'  ]\n",
    "# df= df.drop(columns=unwanted_dates)\n",
    "# unwanted_dates = ['Calendar_last_Scraped','Calendar_last_Scraped_YEAR' , 'Calendar_last_Scraped_YEAR', 'Calendar_last_Scraped_MONTH' , 'Calendar_last_Scraped_DAY' , 'Calendar_last_Scraped_DAYOFWEEK'  ]\n",
    "# df= df.drop(columns=unwanted_dates)\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "add_col = ['Host_Location','Street','Neighbourhood_Cleansed','City','State','Zipcode','Market','Smart_Location','Country_Code','Country']\n",
    "df= df.drop(columns=add_col)\n",
    "\n",
    "df= df.drop(columns='Geolocation')     ### REDUNTENT \n",
    "df= df.drop(columns='Features')    ### TO BE SEEN AFTER WARDS\n",
    "df= df.drop(columns=['License','Jurisdiction_Names','Calendar_Updated'])\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "# Imputation\n",
    "\n",
    "\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numColumns=df.select_dtypes(include=numerics).columns\n",
    "numColumns\n",
    "\n",
    "df['Extra_People']=df['Extra_People'].fillna(0)\n",
    "\n",
    "df[numColumns]=df[numColumns].fillna(df.mean())\n",
    "#numColumns1 = ['Host_Listings_Count' , 'Property_Type' ,'Room_Type' ,'Bed_Type' , 'Guests_Included' ,'Maximum_Nights','Cancellation_Policy']\n",
    "#df[numColumns1]=df[numColumns1].fillna(df.mean())\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "df['Host_Listings_Count']=df['Host_Listings_Count'].fillna(0)\n",
    "\n",
    "df['Property_Type'] = df['Property_Type'].fillna(df['Property_Type'].mode()[0])\n",
    "\n",
    "df['Room_Type'] = df['Room_Type'].fillna(df['Room_Type'].mode()[0])\n",
    "\n",
    "df['Accommodates'] = df['Accommodates'].fillna(df['Accommodates'].mode()[0])\n",
    "\n",
    "df['Bed_Type'] = df['Bed_Type'].fillna(df['Bed_Type'].mode()[0])\n",
    "\n",
    "df['Guests_Included'] = df['Guests_Included'].fillna(df['Guests_Included'].mode()[0])\n",
    "\n",
    "df['Maximum_Nights'] = df['Maximum_Nights'].fillna(df['Maximum_Nights'].mode()[0])\n",
    "\n",
    "df['Cancellation_Policy']=df['Cancellation_Policy'].str.extract(r'(flexible|moderate|strict|no)')\n",
    "df['Cancellation_Policy'] = df['Cancellation_Policy'].fillna(df['Cancellation_Policy'].mode()[0])\n",
    "\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ONe hot encode\n",
    "dfnumhot=pd.get_dummies(df[['Property_Type','Room_Type','Bed_Type','Cancellation_Policy']],\n",
    "                                 prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)\n",
    "\n",
    "dropped_column=['Property_Type','Room_Type','Bed_Type','Cancellation_Policy']\n",
    "df= df.drop(columns=dropped_column) \n",
    "\n",
    "\n",
    "## Concat\n",
    "df = pd.concat([df, dfnumhot], axis=1)\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "#df_hot.select_dtypes(include='object').columns\n",
    "\n",
    "df.Host_Listings_Count.value_counts()\n",
    "df.Host_Listings_Count[df.Host_Listings_Count == 'Private room'] = 1\n",
    "df.Host_Listings_Count[df.Host_Listings_Count == 'Entire home/apt'] = 1\n",
    "\n",
    "df.Accommodates[df.Accommodates == 'yesterday'] = 2\n",
    "df.Accommodates[df.Accommodates == '7 weeks ago'] = 2\n",
    "df.Accommodates[df.Accommodates == 'a week ago'] = 2\n",
    "\n",
    "df.Guests_Included[df.Guests_Included == 'moderate'] = 1\n",
    "df.Guests_Included[df.Guests_Included == 'strict'] = 1\n",
    "\n",
    "\n",
    "\n",
    "df.Maximum_Nights[df.Maximum_Nights == '51.1975959128, 4.4295469314'] = 1\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "df['Host_Listings_Count'] = pd.to_numeric(df['Host_Listings_Count'])\n",
    "df['Accommodates'] = pd.to_numeric(df['Accommodates'])\n",
    "df['Guests_Included'] = pd.to_numeric(df['Guests_Included'])\n",
    "\n",
    "df['Maximum_Nights'] = pd.to_numeric(df['Maximum_Nights'])\n",
    "df['Listing_Type'] = pd.to_numeric(df['Listing_Type'])\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "df.dtypes[df.dtypes == \"O\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('listings.csv', sep = ';' , usecols= ['Amenities','Features'])\n",
    "dataset = dataset[0:100000]\n",
    "print(df.shape)\n",
    "print(dataset.shape)\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "dataset['Amenities']=dataset['Amenities'].str.replace(' ','_')\n",
    "dataset['Amenities']=dataset['Amenities'].str.replace('\\\"','')\n",
    "dataset['Amenities']=dataset['Amenities'].str.replace(',',' ')\n",
    "dataset['Amenities']=dataset['Amenities'].fillna('')\n",
    "dataset['Amenities']=dataset['Amenities'].str.replace('-','_')\n",
    "\n",
    "\n",
    "dataset['Features']=dataset['Features'].str.replace(' ','_')\n",
    "dataset['Features']=dataset['Features'].str.replace('\\\"','')\n",
    "dataset['Features']=dataset['Features'].str.replace(',',' ')\n",
    "dataset['Features']=dataset['Features'].fillna('')\n",
    "dataset['Features']=dataset['Features'].str.replace('-','_')\n",
    "print(df.shape)\n",
    "print(dataset.shape)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectFeatures = CountVectorizer()\n",
    "vectFeatures.fit(dataset['Features'])\n",
    "\n",
    "featuresSplit=vectFeatures.transform(dataset['Features'])\n",
    "#featuresSplit.toarray()\n",
    "\n",
    "vectAmenities = CountVectorizer()\n",
    "vectAmenities.fit(dataset['Amenities'])\n",
    "\n",
    "amenitiesSplit=vectAmenities.transform(dataset['Amenities'])\n",
    "#amenitiesSplit.toarray()\n",
    "print(df.shape)\n",
    "print(dataset.shape)\n",
    "\n",
    "featuresCSV=pd.DataFrame(featuresSplit.toarray(),columns=vectFeatures.get_feature_names())\n",
    "\n",
    "amenitiesCSV=pd.DataFrame(amenitiesSplit.toarray(),columns=vectAmenities.get_feature_names())\n",
    "\n",
    "csvDF=pd.concat([featuresCSV,amenitiesCSV],axis=1)\n",
    "print(csvDF.shape)\n",
    "print(df.shape)\n",
    "print(dataset.shape)\n",
    "\n",
    "df=pd.concat([df.reset_index(drop=True),csvDF],axis=1)\n",
    "df.shape\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_text_subset  = pd.read_csv('df_text_subset.csv')\n",
    "df_text_subset = df_text_subset[0:100000]\n",
    "\n",
    "df_text_subset.shape\n",
    "\n",
    "df = pd.concat([df, df_text_subset], axis=1)\n",
    "df.shape\n",
    "\n",
    "#Listing_Type\n",
    "df = df.dropna(axis=0, subset=['Listing_Type'])\n",
    "df.shape\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "X = df.select_dtypes(include=numerics).drop(columns=['Listing_Type']).values\n",
    "Y = df['Listing_Type'].values\n",
    "scaler = MinMaxScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "X_Columns=df.select_dtypes(include=numerics).drop(columns=['Listing_Type']).columns\n",
    "validation_size = 0.30\n",
    "seed = 100\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\t 69994\n",
      "X_test\t 29998\n",
      "Y_train\t 69994\n",
      "Y_test\t 29998\n"
     ]
    }
   ],
   "source": [
    "print('X_train\\t',len(X_train))\n",
    "print('X_test\\t',len(X_test))\n",
    "print('Y_train\\t',len(Y_train))\n",
    "print('Y_test\\t',len(Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669943137983256\n",
      "[[ 5969   453]\n",
      " [15856 47716]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.93      0.42      6422\n",
      "         1.0       0.99      0.75      0.85     63572\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     69994\n",
      "   macro avg       0.63      0.84      0.64     69994\n",
      "weighted avg       0.92      0.77      0.81     69994\n",
      "\n",
      "--------------------------------------------------------------\n",
      "0.7601173411560771\n",
      "[[ 2461   214]\n",
      " [ 6982 20341]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.92      0.41      2675\n",
      "         1.0       0.99      0.74      0.85     27323\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     29998\n",
      "   macro avg       0.63      0.83      0.63     29998\n",
      "weighted avg       0.92      0.76      0.81     29998\n",
      "\n",
      "____________________________________________________________\n",
      "MODEL IS OVERFITTING\n"
     ]
    }
   ],
   "source": [
    "## BASE LINE\n",
    "# Algorithms\n",
    "model_LR=sklearn.linear_model.LogisticRegression(class_weight='balanced', penalty='l1')\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model_LR.fit(X_train,Y_train)\n",
    "\n",
    "#### Making Prediction ####\n",
    "\n",
    "trainResult_LR=model_LR.predict(X_train)\n",
    "testResult_LR=model_LR.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "### Accuracy score\n",
    "### Confusion Matrix\n",
    "### Classification Report\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(Y_train, trainResult_LR))\n",
    "print(sklearn.metrics.confusion_matrix(Y_train, trainResult_LR))\n",
    "print(sklearn.metrics.classification_report(Y_train, trainResult_LR))\n",
    "print('--------------------------------------------------------------')\n",
    "\n",
    "# Test\n",
    "print(sklearn.metrics.accuracy_score(Y_test, testResult_LR))\n",
    "print(sklearn.metrics.confusion_matrix(Y_test, testResult_LR))\n",
    "print(sklearn.metrics.classification_report(Y_test, testResult_LR))\n",
    "print('____________________________________________________________')\n",
    "if sklearn.metrics.accuracy_score(Y_train, trainResult_LR) > sklearn.metrics.accuracy_score(Y_test, testResult_LR):\n",
    "\n",
    "    print('MODEL IS NOT OVERFITTING')\n",
    "else:\n",
    "    print('MODEL IS  OVERFITTING')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.loc[dataset.Listing_Type=='Good','Listing_Type']=1\n",
    "\n",
    "# dataset.loc[dataset.Listing_Type=='Bad','Listing_Type']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
